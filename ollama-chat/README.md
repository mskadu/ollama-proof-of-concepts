# Chatting with Llama 3.2 running locally on Ollama

All this does is connects to the local LLM and invokes it's [chat completion API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)
