Tool (or function calling)
This implements basic Tool calling with llama3.2 - the LLM, running locally on Ollama. The expected sequence of events is that 

* the LLM receives an input prompt
* It works out the necessary context
* we have a config that gets it to execute our dummy function - relevant to that context
* the LLM receives the output from our function
* The output is included in the output generated by the LLM


# What is tool (or function) calling?
LLMs typically tend to return unstructured data (e.g. natural text) which is challenging to use in applications other than chat. By getting it to invoke functions (or tools/ APIs) we can get the LLM return structured data so it can be parsed by machines (hardware _and_ software) to render web pages, mobile views or even the likes of passing it on to other APIs or store in database columns. Hence the term tool/ function callling.

In the simplified context of our Poc, we are focusing the LLMs ability to invoke "tools or functions" (e.g. an API) based on the context of an incoming request. For example, the user might phrase a question around the weather in a city. The LLM will work out that to generate the response to this input a tool/an API will need to be invoked. And then use the output to generate the text being sent back. The link to the Llama API docs in the references section below has more advanced examples.

As a side note, some LLMs provide native support for such calling by providing specific tokens that can be used to provide the necessary specifications. When a LLM does not provide such a support, it becomes necessary to engineer prompts, followed by fine-tuning and constrained decoding to achieve an equivalent outcome.

# What is fine tuning?
According to [Turing.com](https://www.turing.com/resources/finetuning-large-language-models), this is the process of improving over a LLMs existing training - which might not cover a specific domain/ use case/ task - so it learns how to do so. This is done by exposing the LLM to specific examples to help it understand nuances of the need at hand. 

This helps bridge the gap between general purpose training (e.g. what is a return policy?) and one for a specific domain/ use case/ task (e.g. what is the return policy for _this_ organisation?). 

In our case, we want to the LLM to understand that it needs to invoke our tool/ function instead of doing that it's pre-training would make it do. And that too in the specific context, circumstances, etc.

# What is constrained decoding?
Simply put, this is limiting an LLM from getting creative with the output it generates. This is intended to reduce ambiguity in the final response that the end user receives - and make it as relevant as possible.

In our context, we want the LLM to *not* get creative with the output of our API and limit itself to converting it into a human readable grammar and syntax.

# Reference
* [Function calling - Llama API docs](https://docs.llama-api.com/essentials/function)